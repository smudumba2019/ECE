{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Assignment_04_Instructions.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zmwWs4S9cRIn"},"source":["# ECE 570 Assignment 4 Instructions\n","\n","## **Instructions**\n","This Jupyter notebook document entitled **Assignment_04_Instructions** contains instructions for doing your assignment exercise.\n","A second Jupyter notebook document entited **Assignment_04_Exercise** contains all the exercises that you will need to perform.\n","\n","As you read each section of this instruction, you should try running the associated code snippets. \n","The colaboratory environment allows you to run code snippets locally by clicking on the arrow on the left of the code. This is a wonderful feature that allows you to experiment as you read. You should take advantage of this and experiment and test different ideas, so you can become more familiar with the Python and the Jupyter programing environment. \n","\n","At the end of each sub-section, there will be exercises to perform. \n","You should perform the exercises in the document **Assignment_04_Exercise**, which will contain all your results. \n","You can then hand in your results by printing the **Assignment_04_Exercise** document as a pdf with all code and simulation results included."]},{"cell_type":"markdown","metadata":{"id":"Zz2T0QYYpwVR"},"source":["## Section 1: Using Pytorch to implement autoencoders\n","\n","In previous deep learning tasks, we only focus on how to implement a classifier by using neural networks. However, neural networks can do much more than this. Autoencoders is one of the easist implementation that allows the neural network to output image-like data.  \n","\n","Autoencoders are a general class of neural networks that consist of two components: an encoder and a decoder.\n","The encoder takes the image and encodes it into a low dimensional vector representation. \n","The decoder takes the vector and decompresses it into something close to the original image.\n","Autoencoders have a variety of useful applications including denoising of images.\n","Moreover, they are just cool because they provide a mechanism to represent complex image content as low dimensional vectors.\n","\n","Another important feature of autoencoders is that they use unsupervised training. \n","In other words, they can be trained without training labels.\n","This is important because in many cases it may be difficult to obtain labeled training data or ground truth. \n","\n","In this section, we will learn how to implement a simple autoencoder using a fully connected neural network. \n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mm3tS9wSqPOl"},"source":["Again, we are using our old friend the MNIST data. We first download it and construct data loaders for it. Moreover, we create a variable `device` that sets which device we want the neural network to run onto.\n","\n","**Important**: Be sure to select the GPU device in the colab setup."]},{"cell_type":"code","metadata":{"id":"m1teVgyrP_3W","executionInfo":{"status":"ok","timestamp":1603858931594,"user_tz":240,"elapsed":614,"user":{"displayName":"Sai Mudumba","photoUrl":"","userId":"14622853099905636983"}},"outputId":"b8449d65-72c4-48c7-d91f-bffea486a393","colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["import torchvision\n","import torch\n","\n","transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()]) # Images already in [0,1]\n","print(transform)\n","train_dataset = torchvision.datasets.MNIST('data', train=True, download=True, transform=transform)\n","test_dataset = torchvision.datasets.MNIST('data', train=False, download=True, transform=transform)\n","\n","batch_size_train, batch_size_test = 64, 1000\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size_test, shuffle=False)\n","\n","device = 'cuda' if torch.cuda.is_available()==True else 'cpu'\n","device = torch.device(device)\n","print(f'We are using device name \"{device}\"')"],"execution_count":42,"outputs":[{"output_type":"stream","text":["Compose(\n","    ToTensor()\n",")\n","We are using device name \"cuda\"\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Az5wIlIfqd8-"},"source":["\n","\n","---\n","\n","\n","In the network structure, the neural network itself consists of two parts. The encoding layer and decoding layer. The encoding layer here is a fully connected network that changes the dimension from **[batch_size x 784]** to **[batch_size x 16]**, and the decoding layer works in the exact opposite way, which is a fully connected network that changes the dimension from **[batch_size x 16]** back to **[batch_size x 784]**. Note, by doing this we are compressing our images to 16 instead of 784 features!!  \n","\n","We move our neural network to the device we want to run on simply by applying the function `model.to(device)`.\n","Also, we use a sigmoid activation at the final layer to ensure the output image pixel values are between 0 and 1."]},{"cell_type":"code","metadata":{"id":"hH440RBvYpMu","executionInfo":{"status":"ok","timestamp":1603858931754,"user_tz":240,"elapsed":750,"user":{"displayName":"Sai Mudumba","photoUrl":"","userId":"14622853099905636983"}}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","latent_feature = 16\n","\n","class our_AE(nn.Module):\n","  def __init__(self):\n","    super(our_AE, self).__init__()\n","\n","    # encoder\n","    self.en_fc1 = nn.Linear(in_features=784, out_features=512)\n","    self.en_fc2 = nn.Linear(in_features=512, out_features=latent_feature)\n","\n","    # decoder\n","    self.de_fc1 = nn.Linear(in_features=latent_feature, out_features=512)\n","    self.de_fc2 = nn.Linear(in_features=512, out_features=784)\n","\n","  def forward(self, x):\n","\n","    # encoding layers\n","    x = x.view(-1, 784)\n","    x = F.relu(self.en_fc1(x))\n","    x = F.relu(self.en_fc2(x))\n","\n","    # decoding layers\n","    x = F.relu(self.de_fc1(x))\n","    x = torch.sigmoid(self.de_fc2(x))\n","    x = x.view(-1, 1, 28, 28)\n","    return x\n","\n","\n","AE = our_AE().to(device)\n","optimizer = optim.Adam(AE.parameters(), lr=1e-4)\n","loss_fn = nn.MSELoss(reduction='sum')"],"execution_count":43,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ry9ZcIYlrzY0"},"source":["Here, since the loss function is trying to compare two images. We can select the MSE loss function to calculate the loss. Also, we are selecting the Adam optimizer.\n","\n","\n","---\n","Then, as usual, we define our training and test function for the neural network.  \n","**Note: We move our image tensor to device we want before plugging into the neural network**"]},{"cell_type":"code","metadata":{"id":"F9wKt5_AeQhL","executionInfo":{"status":"ok","timestamp":1603858997429,"user_tz":240,"elapsed":451,"user":{"displayName":"Sai Mudumba","photoUrl":"","userId":"14622853099905636983"}}},"source":["def train(epoch, device):\n","\n","  AE.train() # we need to set the mode for our model\n","\n","  for batch_idx, (images, _) in enumerate(train_loader): # Note that we do not need the labels\n","\n","    optimizer.zero_grad()\n","    images = images.to(device)\n","    output = AE(images)\n","    loss = loss_fn(output, images) # Here is a typical loss function (Mean square error)\n","    loss.backward()\n","\n","    optimizer.step()\n","    print(f'loss: {loss.size()}')\n","    if batch_idx % 10 == 0: # We record our output every 10 batches\n","      train_losses.append(loss.item()/batch_size_train) # item() is to get the value of the tensor directly\n","      train_counter.append(\n","        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n","    if batch_idx % 100 == 0: # We visulize our output every 100 batches\n","      print(f'Epoch {epoch}: [{batch_idx*len(images)}/{len(train_loader.dataset)}] Loss: {loss.item()/batch_size_train}')\n","\n","\n","def test(epoch, device):\n","\n","  AE.eval() # we need to set the mode for our model\n","\n","  test_loss = 0\n","  correct = 0\n","\n","  with torch.no_grad():\n","    for images, _ in test_loader:\n","      images = images.to(device)\n","      output = AE(images)\n","      test_loss += loss_fn(output, images).item()\n","  \n","  test_loss /= len(test_loader.dataset)\n","  test_losses.append(test_loss)\n","  test_counter.append(len(train_loader.dataset)*epoch)\n","\n","  print(f'Test result on epoch {epoch}: Avg loss is {test_loss}')"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q-yDQvopgdt6","executionInfo":{"status":"error","timestamp":1603859003615,"user_tz":240,"elapsed":4021,"user":{"displayName":"Sai Mudumba","photoUrl":"","userId":"14622853099905636983"}},"outputId":"a4cdb576-deb6-444f-a104-4959dd1be617","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["train_losses = []\n","train_counter = []\n","test_losses = []\n","test_counter = []\n","max_epoch = 2\n","\n","for epoch in range(1, max_epoch+1):\n","  train(epoch, device=device)\n","  test(epoch, device=device)"],"execution_count":49,"outputs":[{"output_type":"stream","text":["loss: torch.Size([])\n","Epoch 1: [0/60000] Loss: 22.498844146728516\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","Epoch 1: [6400/60000] Loss: 23.13965606689453\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","Epoch 1: [12800/60000] Loss: 22.08038330078125\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","Epoch 1: [19200/60000] Loss: 20.844318389892578\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","Epoch 1: [25600/60000] Loss: 22.532379150390625\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n","loss: torch.Size([])\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-49-747b0499a90d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-48-f30057db2208>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mAE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# we need to set the mode for our model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Note that we do not need the labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \"\"\"\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pic should be 2/3 dimensional. Got {} dimensions.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;31m# handle numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"iSJvS-Cu5hkY"},"source":["Then, we can print our result by simply selecting several images from the dataset and then forwarding the images using autoencoders.  Note we can move the images on the GPU back to the CPU using the `.cpu()` function of PyTorch tensors."]},{"cell_type":"code","metadata":{"id":"BbuYRkSDmiBV","executionInfo":{"status":"aborted","timestamp":1603858941936,"user_tz":240,"elapsed":10865,"user":{"displayName":"Sai Mudumba","photoUrl":"","userId":"14622853099905636983"}}},"source":["import matplotlib.pyplot as plt\n","\n","batch_idx, (images, _) = next(enumerate(test_loader))\n","images = images.to(device)\n","output = AE(images).cpu().detach()\n","images = images.cpu()\n","\n","print(images.size(), output.size())\n","\n","fig, ax = plt.subplots(2,4)\n","fig.set_size_inches(12,6)\n","\n","for idx in range(4):\n","  ax[0,idx].imshow(images[idx][0], cmap='gray')\n","  ax[1,idx].imshow(output[idx][0], cmap='gray')\n","\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xdV_V6kp-WcR"},"source":["Note: We are only using latent space with size 16 compared to the original data which is 28*28=784. We have achieved a compression rate of 17/784 = 0.02!!!"]},{"cell_type":"markdown","metadata":{"id":"1OUl4Xu-ODh_"},"source":["## Section 2: Convolutional autoencoders (and transpose convolutions)\n","\n","### Encoders via strided convolutions (that downsample)\n","Convolutions with a stride greater than 1 will generally reduce the width and height of the image.\n","Thus, convolutions can be seen as a type of downsampling.\n","One way to construct the encoder is to downsample the images via convolutions.\n","Here we give an example of using two convolutional layers with stride of 2 to reduce the dimensionality."]},{"cell_type":"code","metadata":{"id":"40XxHTrtODiA","executionInfo":{"status":"aborted","timestamp":1603858941939,"user_tz":240,"elapsed":10839,"user":{"displayName":"Sai Mudumba","photoUrl":"","userId":"14622853099905636983"}}},"source":["batch_size = 10\n","n_channels = 3\n","n_latent = 4\n","image_size = 4\n","conv1 = nn.Conv2d(n_channels, 5, kernel_size=2, stride=2)\n","conv2 = nn.Conv2d(5, n_latent, kernel_size=2, stride=2)\n","\n","x = torch.randn((batch_size, n_channels, image_size, image_size)) # \n","print(f'x shape = {x.shape}')\n","x = conv1(x)\n","print(f'x shape = {x.shape}')\n","x = conv2(x)\n","print(f'x shape = {x.shape}')\n","x = x.view(batch_size, -1)\n","print(f'x shape = {x.shape}') # 4 dimensional latent space based on channels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ONB3FMnsODiC"},"source":["### Decoders via transpose convolution / fractionally strided convolution (that upsample)\n","\n","For the decoders, we need to upsample to go from a latent dimension back to the original dimension.\n","Thus, we need something like convolutions but that can upsample rather than downsample.\n","The transposed convolution does exactly this.  The number of input and output channels is similar to the normal convolution, however, the operation does the \"transpose\" of the convolutions such that a stride of 2 actually increases the output size rather than decreasing.\n","\n","This is similar to the transpose of a matrix multiplication.  For example, if $A \\in R^{20,50}$ and $x\\in R^{50}$, then $y = Ax \\in R^{20}$--thus going from 50 to 20 dimensions.\n","However, if we multiply $A^T$ and $y \\in R^{20}$, then we get $z = A^Ty \\in R^{50}$--thus going from 20 to 50 dimensions.\n","\n","Note that transpose convolutions are not inverses but rather just upsampling operations (similar to the fact that $A^T$ is not the inverse of $A$ except if $A$ is orthogonal.\n","\n","While you don't need more information than above to do the exercises, you can see https://arxiv.org/pdf/1603.07285v1.pdf chapter 4 for more information on transpose convolutions.\n","\n","In practice, we can easily create these transpose convolutions in PyTorch to upsample from the latent space to the original image shape as we show in the example below.\n","Note that we switch the order of the convolutions and the input/output number of channels, but the transpose convolutions have the same kernel_size and stride as the convolutions original convolutions.\n","\n","For convolutional autoencoders, usually the encoders use convolutions and the decoder uses transpose convolutions in the reverse order to get back the original shape."]},{"cell_type":"code","metadata":{"id":"mgI08sSjODiC","executionInfo":{"status":"aborted","timestamp":1603858941944,"user_tz":240,"elapsed":10790,"user":{"displayName":"Sai Mudumba","photoUrl":"","userId":"14622853099905636983"}}},"source":["tran_conv2 = nn.ConvTranspose2d(n_latent, 5, kernel_size=2, stride=2)\n","tran_conv1 = nn.ConvTranspose2d(5, n_channels, kernel_size=2, stride=2)\n","\n","z = x\n","print(f'z shape = {z.shape}')\n","z = z.view(batch_size, n_latent, 1, 1)\n","print(f'z shape = {z.shape}')\n","z = tran_conv2(z)\n","print(f'z shape = {z.shape}')\n","z = tran_conv1(z)\n","print(f'z shape = {z.shape}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s-9FIQqwHJoQ"},"source":["## Section 3: Install extra packages in Colab.\n","\n","The `!` in Jupyter notebooks executes a bash shell command so you can do things like installing Python modules using `pip`, cloning GitHub repositories using `!git clone ...` or showing the contents of the current directory via `!ls`.  \n","\n","You probably need packages that is nor avalible here in the colab environment. Here is a example on how to implement this using `pip`.\n","\n","---\n","\n","If we need to import the package called `torchsummary`, we only need to use pip to install the package (this will have to be run every time you run the notebook as colab destroys the environment when you close the notebook."]},{"cell_type":"code","metadata":{"id":"l8dscz62ANfe","executionInfo":{"status":"aborted","timestamp":1603858941945,"user_tz":240,"elapsed":10775,"user":{"displayName":"Sai Mudumba","photoUrl":"","userId":"14622853099905636983"}}},"source":["!pip install torchsummary"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"euM5VqO6Amda","executionInfo":{"status":"aborted","timestamp":1603858941946,"user_tz":240,"elapsed":10762,"user":{"displayName":"Sai Mudumba","photoUrl":"","userId":"14622853099905636983"}}},"source":["from torchsummary import summary\n","summary(AE, (1,784))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ExYV_yvadQE6"},"source":["You can actually use the summary function a lot. It helps print your model in a more nicer form.\n"]},{"cell_type":"markdown","metadata":{"id":"sRppW2GazXtb"},"source":["## Section 4: Customizable transform function and dataloader\n","Usually when we want to preprocess our dataset. We apply the corresponding transform functions in the data download step. However, what if we want to write our own transform functions? For example, if we want to flip the pixel values how do we apply such transform to the dataset. It turns out you only need to define a new transform class that has the following functions:\n","* `__init__`: This function initializes your transformer (usually to set parameters of the transformer)\n","* `__call__`: This function should contain your transform operation\n","Here is an example on how to create a flip value transform:\n","\n"]},{"cell_type":"code","metadata":{"id":"M3-3E1wgDLsv","executionInfo":{"status":"aborted","timestamp":1603858941946,"user_tz":240,"elapsed":10746,"user":{"displayName":"Sai Mudumba","photoUrl":"","userId":"14622853099905636983"}}},"source":["import torchvision\n","import torch\n","import matplotlib.pyplot as plt\n","\n","class FlipValue(object):\n","  def __init__(self, max_value=1):\n","    self.max_value = max_value\n","\n","  def __call__(self, tensor):\n","    tensor = self.max_value - tensor\n","    return tensor\n","\n","transform_flip = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), FlipValue(1)])\n","ans = FlipValue()\n","ans(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"71hVkIxZ3fXz"},"source":["Note that we can just concat our transform with other transforms. \n","\n","---\n","\n","Now, we want to create a paired dataset that extract both the flipped dataset and original dataset at the same time when we call the dataloader. Again, this can be done by creating a new dataset class with the following functions:\n","* `__init__`: This function is needed to initilize your dataset.\n","* `__getitem__`: This function is used when you call the class by an index (e.g., `dataset[10]`) .\n","* `__len__`: This function is called when you apply the `len()` function to your class.\n","\n","Below is one way to concat two datasets and form a paired dataset.\n"]},{"cell_type":"code","metadata":{"id":"xWBKXGa-2z-b","executionInfo":{"status":"aborted","timestamp":1603858941947,"user_tz":240,"elapsed":10732,"user":{"displayName":"Sai Mudumba","photoUrl":"","userId":"14622853099905636983"}}},"source":["transform_flip = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), FlipValue(1)])\n","transform_original = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n","\n","train_dataset_flipped = torchvision.datasets.MNIST('data', train=True, download=True, transform=transform_flip)\n","train_dataset_original = torchvision.datasets.MNIST('data', train=True, download=True, transform=transform_original)\n","\n","class ConcatDataset(torch.utils.data.Dataset):\n","  ###########################   <YOUR CODE>  ############################\n","  def __init__(self, *datasets):\n","    self.datasets = datasets\n","\n","  def __getitem__(self, i):\n","    return tuple(d[i] for d in self.datasets)\n","\n","  def __len__(self):\n","    return min(len(d) for d in self.datasets)\n","  #########################  <END YOUR CODE>  ############################\n","\n","train_loader = torch.utils.data.DataLoader(ConcatDataset(train_dataset_flipped, train_dataset_original),\n","                      batch_size=10, shuffle=True)\n","_, (fliped, original) = next(enumerate(train_loader))\n","fig,ax = plt.subplots(2,3)\n","fig.set_size_inches(12,8)\n","for idx in range(3):\n","  ax[0,idx].imshow(fliped[0][idx][0], cmap='gray')\n","  ax[0,idx].set_title(f'label is {fliped[1][idx]}')\n","  ax[1,idx].imshow(original[0][idx][0], cmap='gray')\n","  ax[1,idx].set_title(f'label is {original[1][idx]}')\n","fig.show()"],"execution_count":null,"outputs":[]}]}